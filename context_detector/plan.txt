# Context Dependency Detection with Sentence Transformers - Plan

## Project Overview
**Author:** Calvin Seamons
**Format:** Markdown analysis document
**Location:** `docs/context_dependency_analysis.md`
**Purpose:** School project comparing sentence transformer architectures and approaches for detecting context-dependent queries

---

## Project Focus

**Core Question:** Can sentence transformers accurately detect if a follow-up query depends on conversation context?

**What we're comparing:**
1. **Approaches:** Zero-shot similarity vs fine-tuned classification
2. **Models:** Different sizes/architectures (17M → 33M params)
3. **Performance:** Accuracy, inference speed, resource usage

**Practical motivation:** In a conversational RAG system, context-dependent queries like "What's its damage?" need rewriting before classification, while standalone queries don't.

---

## Document Structure

### 1. Introduction
- Problem: Detecting context dependency in conversational queries
- Goal: Compare architectures and approaches

### 2. Application Context (Brief)
- ShadowScribe 2.0: D&D assistant with RAG (motivation for this research)
- Example: "What is Fireball?" → "What's its damage?" (context-dependent)
- Example: "What is Fireball?" → "How does grappling work?" (standalone)

### 3. Approach Comparison

#### Approach A: Zero-Shot Similarity
- No training required
- Method: `cosine(embed(Q2), embed(Q1+A1)) > threshold`
- Hypothesis: High similarity = context-dependent
- Limitations: Won't catch implicit dependencies

#### Approach B: Fine-Tuned Classifier
- Add classification head to sentence transformer
- Input: `[CLS] Q1 [SEP] A1 [SEP] Q2 [SEP]`
- Output: Binary (0=standalone, 1=context-dependent)
- Requires training data

### 4. Model Comparison

| Model | Params | Size | Expected Latency |
|-------|--------|------|------------------|
| `paraphrase-MiniLM-L3-v2` | 17M | 61MB | ~5ms |
| `all-MiniLM-L6-v2` | 22M | 80MB | ~8ms |
| `all-MiniLM-L12-v2` | 33M | 120MB | ~15ms |

For each model, test:
- Zero-shot similarity accuracy
- Fine-tuned classifier accuracy
- Inference latency
- Memory footprint

### 5. Training Data Generation
- Base: 37 existing test pairs (34 context-dependent, 3 standalone)
- Expansion: Generate from ShadowScribe knowledge base
  - Character data queries
  - Session notes queries
  - Rulebook queries
- Augmentation techniques:
  - Synonym replacement
  - Pronoun variation
  - Entity swapping
- Target: 500+ labeled examples

### 6. Evaluation Metrics
- Accuracy, Precision, Recall, F1
- False Positive Rate (standalone classified as dependent → unnecessary rewriter call)
- False Negative Rate (dependent classified as standalone → broken query passed through)
- Inference time per query
- Memory usage

### 7. Results & Analysis
- Zero-shot vs fine-tuned comparison
- Model size vs accuracy tradeoff
- Threshold tuning for zero-shot approach
- Confusion matrices
- Example successes and failures

### 8. Conclusion
- Which approach works best (zero-shot vs fine-tuned)
- Model size vs accuracy tradeoffs
- Key findings about sentence transformer capabilities for this task

---

## Implementation Steps

### Phase 1: Setup & Zero-Shot Testing
1. Create test harness for sentence transformer evaluation
2. Implement zero-shot similarity approach
3. Test with existing 37 pairs across 3 models
4. Find optimal similarity threshold

### Phase 2: Training Data Generation
1. Explore ShadowScribe knowledge base
2. Generate diverse Q1/A1/Q2 triplets
3. Label as context-dependent or standalone
4. Apply augmentation
5. Create train/val/test splits

### Phase 3: Fine-Tuning
1. Add classification head to each model
2. Train on generated dataset
3. Evaluate on held-out test set
4. Compare to zero-shot baseline

### Phase 4: Analysis & Report
1. Compile all results
2. Create comparison visualizations
3. Write analysis document
4. Draw conclusions

---

## Files to Create
- `docs/context_dependency_analysis.md` - Main analysis report
- `context_detector/` - Experiment code
  - `zero_shot.py` - Similarity-based detection
  - `classifier.py` - Fine-tuned classifier
  - `data_generator.py` - Training data generation
  - `evaluate.py` - Evaluation harness
  - `train.py` - Fine-tuning script
  - `training_data/` - Generated examples
  - `results/` - Experiment results (JSON)
